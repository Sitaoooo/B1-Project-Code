{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sitaoooo/B1-Project-Code/blob/master/regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "# standardization subtracting the mean and dividing by the standard deviation\n",
        "X = StandardScaler().fit_transform(X)\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.FloatTensor(y_test)"
      ],
      "metadata": {
        "id": "D2PQCxZRojUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTihFat7GmDm"
      },
      "outputs": [],
      "source": [
        "# Linear Regression Model\n",
        "class LR(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LR, self).__init__()\n",
        "        # this defines a linear operation (i.e. a matrix with specified dimension)\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # self.linear(x) will process x by the above defined linear operation\n",
        "        # and calculate the output after the operation\n",
        "        return self.linear(x)\n",
        "\n",
        "# TODO: Neural Network: Please complete to finish Q1\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(NN, self).__init__()\n",
        "        self.layer1 =\n",
        "        self.layer2 =\n",
        "\n",
        "    # TODO: fill the below function; torch.relu() is needed\n",
        "    # The forward function defines how your model processes input data to produce an output.\n",
        "    # This is where the actual computation of the model takes place,\n",
        "    # applying operations defined in the constructor (__init__ method) to the input tensors.\n",
        "    # To define an one hidden layer NN with relu activation,\n",
        "    # you need to apply a linear transformation first, then apply relu activation;\n",
        "    # then apply another linear transformation to produce the output.\n",
        "    def forward(self, x):\n",
        "        x =\n",
        "        return self.layer2(x)\n",
        "\n",
        "def train_and_evaluate(model, X_train, y_train, X_test, y_test, learning_rate, num_epochs, lambda_reg=0.):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        l2_reg = torch.tensor(0.)\n",
        "        for param in model.parameters():\n",
        "            # requries_grad is a flag that shows whether\n",
        "            # the gradients w.r.t the tensor are calculated or not.\n",
        "            # If requires_grad is True it will mean that PyTorch will keep\n",
        "            # of the gradient of the [output] w.r.t. the Tensor varaiable.\n",
        "            if param.requires_grad and len(param.shape)>1 and param.shape[1]>1:\n",
        "                l2_reg += torch.sum(param**2)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = loss + lambda_reg * l2_reg\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # We do not need to call the forward method directly.\n",
        "            # Instead, we call the model object itself with the input data as an argument,\n",
        "            # like so: output = model(input_tensor).\n",
        "            # When you do this, PyTorch internally calls the forward method of your model with the input tensor.\n",
        "            test_outputs = model(X_test)\n",
        "            test_loss = criterion(test_outputs, y_test)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch}/{num_epochs}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
        "\n",
        "    return train_losses, test_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Please complete the above NN class. Your neural network should have a single hidden layer with Relu activation. Then train it by running the following block of code. You should see a learning curve looks like:"
      ],
      "metadata": {
        "id": "a8a5vuU_bLxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[click to open fig](https://drive.google.com/file/d/1VcCke3nC5mJrkKk255Jb5ffksagYBFPc/view?usp=sharing)"
      ],
      "metadata": {
        "id": "7XFSpdbpg0qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_eps = 500\n",
        "\n",
        "# Train and evaluate Neural Network without regularization\n",
        "nn_model = NN(X_train.shape[1], hidden_size=128)\n",
        "train_losses_nn, test_losses_nn = train_and_evaluate(nn_model, X_train_tensor, y_train_tensor,\n",
        "                                                     X_test_tensor, y_test_tensor, learning_rate=0.001, num_epochs=n_eps)\n",
        "\n",
        "\n",
        "# Setting the line width\n",
        "line_width = 4\n",
        "\n",
        "# Figure for Neural Network without Regularization\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(train_losses_nn, label='Neural Network - Train', linestyle='-', linewidth=line_width)\n",
        "plt.plot(test_losses_nn, label='Neural Network - Test', linestyle=':', linewidth=line_width)\n",
        "plt.title('Neural Network Without Regularization')\n",
        "plt.ylim(0, 3)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3RBgx5LaNUvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. You are given the following figure of learning curve, which is generated by one of the following settings (you can assume other settings such as NN size, minibatch size and optimizer are the same as above):\n",
        "\n",
        "*   NN with learning rate $0.001$ with regularization weight 0.1\n",
        "*   NN with learning rate $0.01$ without regularization\n",
        "*   LR (linear regression) with learning rate $0.1$ without regularization\n",
        "*   LR (linear regression) with learning rate $0.1$ with regularization weight 100\n",
        "*   LR (linear regression) with learning rate $0.1$ with regularization weight 0.1\n",
        "\n",
        "Try to guess which setting was used to get the figure. Why?"
      ],
      "metadata": {
        "id": "iDHaGKM5TEnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[click to open fig](https://drive.google.com/file/d/1jjuRIGOmT6M4KNQjMWNor_bdc0XEP7lu/view?usp=sharing)"
      ],
      "metadata": {
        "id": "4q_RMYzxbeVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Please add code below to train the guessed algorithm & hyperparameter setting. Make sure you can reproduce the above figure."
      ],
      "metadata": {
        "id": "cV8z51ffUQYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_eps = 500\n",
        "\n",
        "# TODO: Train and evaluate xxx, please fill the below and set learning rate, lambda reg.\n",
        "model =\n",
        "train_losses_nn, test_losses_nn = train_and_evaluate(model, X_train_tensor, y_train_tensor,\n",
        "                                                     X_test_tensor, y_test_tensor, learning_rate= , num_epochs=n_eps, lambda_reg= )\n",
        "\n",
        "\n",
        "# Setting the line width\n",
        "line_width = 4\n",
        "\n",
        "# Figure for learning curves of your algorithm\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(train_losses_nn, label=' - Train', linestyle='-', linewidth=line_width)\n",
        "plt.plot(test_losses_nn, label=' - Test', linestyle=':', linewidth=line_width)\n",
        "plt.title(' ')\n",
        "plt.ylim(0, 3)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CvVUVHJlTTY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. In the above train_and_evaluation function, within the second for loop. What do you think the purpose of the if condition len(param.shape)>1 and param.shape[1]>1 in **linear regression**? Why do we need it?"
      ],
      "metadata": {
        "id": "Gm4JoxrZiPrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3pbGZZbngY88"
      }
    }
  ]
}