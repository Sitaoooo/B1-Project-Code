{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sitaoooo/B1-Project-Code/blob/master/laplace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Laplace approximation methods. Please complete the code below to obtain the Laplace approximate distribution of the MAP estimate of training parameters (slope and mean).\n",
        "\n",
        "What do you observe when you increase the number of samples (i.e., n_samples below)?"
      ],
      "metadata": {
        "id": "EOufQ38OnM_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Is the result of theta_map (i.e., the MAP estimation of theta) the same as the Ordinary Least Squares (OLS) solution with L2 regularization? Why?"
      ],
      "metadata": {
        "id": "Q9AjJ8wFsk71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YymHbr6Nmhm8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate synthetic regression dataset, try to increase n_samples to sth larger, i.e., 500, 10000, 20000, ...\n",
        "n_samples = 200\n",
        "X = np.random.uniform(-2, 10, size=n_samples)\n",
        "slope_mean = 3\n",
        "intercept_mean = 8\n",
        "true_slope = slope_mean\n",
        "true_intercept = intercept_mean\n",
        "noise = np.random.normal(0, 5, n_samples)  # Gaussian noise\n",
        "y = true_slope * X + true_intercept + noise\n",
        "\n",
        "# Plotting the dataset\n",
        "plt.scatter(X, y)\n",
        "plt.xlabel('Feature X')\n",
        "plt.ylabel('Target y')\n",
        "plt.title('Synthetic Linear Regression Dataset')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# TODO: Define the linear model function\n",
        "def linear_model(theta, X):\n",
        "    return # fill here\n",
        "\n",
        "# Define the log-likelihood function\n",
        "def log_likelihood(theta, X, y):\n",
        "    predictions = linear_model(theta, X)\n",
        "    return -0.5 * np.sum((y - predictions) ** 2)\n",
        "\n",
        "# Define the log-prior function (Gaussian) with unit variance\n",
        "def log_prior(theta, mean=0):\n",
        "    return -0.5 * np.sum((theta[1] - mean) ** 2)\n",
        "\n",
        "# TODO: Define the log-posterior function\n",
        "def log_posterior(theta, X, y):\n",
        "    return # fill here\n",
        "\n",
        "# Compute MAP estimate\n",
        "X_with_intercept = np.column_stack((np.ones(n_samples), X))\n",
        "initial_theta = np.zeros(2)\n",
        "# below minimize function will try to minimize the neg-log function over theta\n",
        "# starting from the initial value initial_theta\n",
        "result = minimize(lambda theta: -log_posterior(theta, X_with_intercept, y), initial_theta)\n",
        "theta_map = result.x\n",
        "\n",
        "# Compute Hessian matrix\n",
        "def compute_hessian(X):\n",
        "    mat = np.array([[0,0],[0,1]])\n",
        "    # the @ sign is another notation for matrix product, X.T is the transpose of X\n",
        "    return X.T @ X + mat\n",
        "\n",
        "# TODO: Please fill below two lines\n",
        "Hessian =\n",
        "posterior_covariance ="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the approximated posterior and true distributions\n",
        "theta_values = np.linspace(theta_map[0] - 2, theta_map[0] + 2, 100)\n",
        "plt.plot(theta_values, norm.pdf(theta_values, theta_map[0], np.sqrt(posterior_covariance[0, 0])), label='Approximate Posterior (Intercept)')\n",
        "plt.axvline(x=intercept_mean, color='red', linestyle='--', label='True Intercept')\n",
        "plt.xlabel('Intercept Value')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.title('Comparison of Approximated and True Intercept Distributions')\n",
        "plt.show()\n",
        "\n",
        "theta_values = np.linspace(theta_map[1] - 2, theta_map[1] + 2, 100)\n",
        "plt.plot(theta_values, norm.pdf(theta_values, theta_map[1], np.sqrt(posterior_covariance[1, 1])), label='Approximate Posterior (Slope)')\n",
        "plt.axvline(x=slope_mean, color='red', linestyle='--', label='True Slope')\n",
        "plt.xlabel('Slope Value')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.title('Comparison of Approximated and True Slope Distributions')\n",
        "plt.show()\n",
        "\n",
        "# Print the results\n",
        "print(\"True Parameters mean: Intercept:\", intercept_mean, \", Slope:\", slope_mean)\n",
        "print(\"MAP Estimates:\", theta_map)\n",
        "print(\"Posterior Covariance Matrix:\\n\", posterior_covariance)"
      ],
      "metadata": {
        "id": "bCS3QAqYr5IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TJ69Tk9vtMPn"
      }
    }
  ]
}