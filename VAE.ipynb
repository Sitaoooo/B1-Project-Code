{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sitaoooo/B1-Project-Code/blob/master/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you implement a very important and popular generative model: variational autoencoder. One main goal of an autoencoder is to learn somewhat useful representation of data (by learning an encoder of the data). Particularly, variational autoencoder (VAE) is additionally used for generative modeling, meaning they can generate new data points that resemble the original input data. After training, you can sample from the latent space to generate new data instances. This is particularly useful in fields like image generation, where VAEs can create new images that share characteristics with a training set."
      ],
      "metadata": {
        "id": "M1Ku9hmQ5pxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it comes to generative models, one can immediately think of learning a distribution $p(x)$ so that we can draw samples from such a distribution. However, it could be challenging in general, as the distribution may not even have a nice format like those well-known Gaussian, uniform, etc., and it is also challenging to draw samples from a distribution of a high-dimensional random variable."
      ],
      "metadata": {
        "id": "rp8mFU7VN7L5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common way to learn a complicated distribution function is to first introduce some latent variable $z$ that is assumed to be from some simple distribution such as a Gaussian $N(z; 0, I)$, then we can assume our data $x$ is sampled from $x\\sim N(x; f(z), g(z)^2)$. We need to:\n",
        "\n",
        "$\\max_\\theta E_x[\\log p_\\theta (x)]$, note that\n",
        "\n",
        "$\\log p_\\theta (x) = \\log \\int_z p_\\theta(x|z)p(z) dz$\n",
        "\n",
        "$\n",
        "= \\log \\int_z p_\\theta(x|z)p(z) \\frac{q_\\phi(z|x)}{q_\\phi(z|x)} dz \\\\\n",
        "= \\log E_{q_\\phi}[p_\\theta(x|z)p(z)/q_\\phi(z|x)] \\\\\n",
        "\\ge E_{q_\\phi}[\\log p_\\theta(x|z)p(z)/q_\\phi(z|x)], \\text{by Jensen inequality}\\\\\n",
        "= E_{q_\\phi}[\\log p_\\theta(x|z)] + E_{q_\\phi}[\\log p(z)/q_\\phi(z|x)] \\\\\n",
        "= E_{q_\\phi}[\\log p_\\theta(x|z)] - KL(q_\\phi(z|x) || p(z))\n",
        "$\n",
        "\n",
        "which is known as the Evidence Lower BOund (ELBO)."
      ],
      "metadata": {
        "id": "E7aTM_5lEr2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conveniently compute gradient, the first term can be rewritten as (called reparameterization):\n",
        "\n",
        "$E_{\\epsilon\\sim N(0, I)}[\\log p_\\theta(x|z=\\sigma_\\phi(x) \\epsilon + \\mu_\\phi(x))]$"
      ],
      "metadata": {
        "id": "7JVP6MwbEvy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above hints, please complete the following."
      ],
      "metadata": {
        "id": "sbizgFhtpb3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Please fill the reparameterize function.\n",
        "\n",
        "Q2. Please fill the decode function.\n",
        "\n",
        "Q3. Please fill the forward function.\n",
        "\n",
        "Q4. Please fill the loss function. You probably want to take a look [wiki page about KL](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions)\n",
        "\n",
        "Q5. Please fill the final block of code and then run the code so you can draw samples from the VAE after training."
      ],
      "metadata": {
        "id": "XRl5IlW9oeWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYx_pChlzEr7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc21 = nn.Linear(128, 20)  # Mean\n",
        "        self.fc22 = nn.Linear(128, 20)  # Log variance\n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(20, 128)\n",
        "        self.fc4 = nn.Linear(128, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        # TODO: return what?\n",
        "        return\n",
        "\n",
        "    # TODO: Please fill the decode function\n",
        "    def decode(self, z):\n",
        "        return\n",
        "\n",
        "    # TODO: complete this forward function\n",
        "    def forward(self, x):\n",
        "        mu, logvar =\n",
        "        z =\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "# TODO: complete this loss function\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VAE()\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = vae(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch}, Loss: {train_loss / len(train_loader.dataset)}')\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # TODO: add code to the following two lines\n",
        "    # so you can draw samples from the autoencoder\n",
        "    z =\n",
        "    sample =\n",
        "    sample = sample.view(64, 1, 28, 28)\n",
        "    grid_img = torchvision.utils.make_grid(sample, nrow=8)\n",
        "    plt.imshow(grid_img.permute(1, 2, 0))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "X5RBD5wtaiGv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}